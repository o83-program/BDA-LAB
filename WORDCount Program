package wordCount;
import java.io.IOException;
import org.apache.hadoop.conf.Configured; import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat; import org.apache.hadoop.mapred.FileOutputFormat; import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf; import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
public class WCDriver extends Configured implements Tool { public int run(String args[]) throws IOException29
{
if (args.length < 2)
{
System.out.println("Please give valid inputs"); return -1;
}
JobConf conf = new JobConf(WCDriver.class); FileInputFormat.setInputPaths(conf, new Path(args[0])); FileOutputFormat.setOutputPath(conf, new Path(args[1])); conf.setMapperClass(WCMapper.class); conf.setReducerClass(WCReducer.class);
 
conf.setMapOutputKeyClass(Text.class); conf.setMapOutputValueClass(IntWritable.class); conf.setOutputKeyClass(Text.class); conf.setOutputValueClass(IntWritable.class); JobClient.runJob(conf);
return 0;
}
// Main Method
public static void main(String args[]) throws Exception
{
int exitCode = ToolRunner.run(new WCDriver(), args); System.out.println(exitCode);
}
}
//Mapper Code package wordCount;30
import java.io.IOException;
import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase; import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector; import org.apache.hadoop.mapred.Reporter;
public class WCMapper extends MapReduceBase implements Mapper<LongWritable,Text, Text, IntWritable> {
// Map function
public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output,
Reporter
rep) throws IOException
 
{
String line = value.toString();
// Splitting the line on spaces for (String word : line.split(" "))
{
if (word.length() > 0)
{
output.collect(new Text(word), new IntWritable(1));
}
}
}
}
//Reducer Code31 package wordCount;
import java.io.IOException; import java.util.Iterator;
import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase; import org.apache.hadoop.mapred.OutputCollector; import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;
public class WCReducer extends MapReduceBase implements Reducer<Text,IntWritable, Text, IntWritable> {
// Reduce function
public void reduce(Text key, Iterator<IntWritable> value, OutputCollector<Text, IntWritable> output,Reporter rep) throws IOException
{
int count = 0;
// Counting the frequency of each words
 
while (value.hasNext())
{
IntWritable i = value.next(); count += i.get();
}
output.collect(key, new IntWritable(count));
}
}
//Hadoop Commands
hduser@bmsce-Precision-T1700:~$ start-all.sh
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh Starting namenodes on [localhost]
hduser@localhost's password:
localhost: namenode running as process 10473. Stop it first. hduser@localhost's password:
localhost: datanode running as process 10644. Stop it first. Starting secondary namenodes [0.0.0.0]
hduser@0.0.0.0's password:
0.0.0.0: secondarynamenode running as process 10857. Stop it first. starting yarn daemons
resourcemanager running as process 9796. Stop it first. hduser@localhost's password:
localhost: nodemanager running as process 10160. Stop it first. hduser@bmsce-Precision-T1700:~$ jps
10160 NodeManager
7441 org.eclipse.equinox.launcher_1.5.600.v20191014-2022.jar
9796 ResourceManager
12692 org.eclipse.equinox.launcher_1.5.600.v20191014-2022.jar
10644 DataNode
10857 SecondaryNameNode
 
10473 NameNode
15100 Jps
hduser@bmsce-Precision-T1700:~$ hadoop fs -ls / Found 10 items33
drwxr-xr-x - hduser supergroup 0 2023-01-23 09:52 /gou
drwxr-xr-x - hduser supergroup 0 2023-01-23 10:33 /har
drwxr-xr-x - hduser supergroup 0 2023-04-14 10:50 /input
drwxr-xr-x - hduser supergroup 0 2023-05-23 09:58 /output1
drwxr-xr-x - hduser supergroup 0 2023-01-23 15:57 /output2
drwxr-xr-x - hduser supergroup 0 2023-01-15 10:27 /rgs
drwxr-xr-x - hduser supergroup 0 2023-01-23 11:09 /stud
drwxr-xr-x - hduser supergroup 0 2023-05-23 15:50 /testing
drwxrwxr-x - hduser supergroup 0 2023-05-23 11:24 /tmp
drwxr-xr-x - hduser supergroup 0 2023-05-01 16:03 /user
hduser@bmsce-Precision-T1700:~$ hadoop fs -mkdir /1BM20CS187 hduser@bmsce-Precision-T1700:~$ hadoop fs -copyFromLocal
/home/hduser/Desktop/sample.txt
/1BM19CS167/test.txt
hduser@bmsce-Precision-T1700:~$ hdfs dfs -cat /1BM20CS187/test.txt hi how are you
how is your job how is your family how is your brother how is your sister
hduser@bmsce-Precision-T1700:~$ hadoop jar /home/hduser/Documents/wordCount.jar wordCount.WCDriver /1BM20CS187/test.txt /1BM19CS167/output
File System Counters
FILE: Number of bytes read=8614 FILE: Number of bytes written=510599 FILE: Number of read operations=0
 
FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=178 HDFS: Number of bytes written=69 HDFS: Number of read operations=13
HDFS: Number of large read operations=0 HDFS: Number of write operations=4 Map-Reduce Framework
Map input records=5 Map output records=20 Map output bytes=169
Map output materialized bytes=215 Input split bytes=87
Combine input records=0 Combine output records=0 Reduce input groups=10 Reduce shuffle bytes=215 Reduce input records=20 Reduce output records=10 Spilled Records=40 Shuffled Maps =1
Failed Shuffles=0 Merged Map outputs=1 GC time elapsed (ms)=1 CPU time spent (ms)=0
Physical memory (bytes) snapshot=0 Virtual memory (bytes) snapshot=0
Total committed heap usage (bytes)=47185920038 Shuffle Errors
BAD_ID=0
 
CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0
File Input Format Counters Bytes Read=89
File Output Format Counters Bytes Written=69
0
hduser@bmsce-Precision-T1700:~$ hdfs dfs -cat /1BM19CS167/output/part-00000 are	1
brother 1
family 1
hi 1
how 5
is 4
job 1
sister 1
you 1
your 4
